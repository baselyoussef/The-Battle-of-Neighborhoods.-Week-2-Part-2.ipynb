{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "## Part 2 Web scrapping of Population and Demographics data of New York city from Wikipedia"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**A : POPULATION DATA**"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Web scrapping of Population data from wikipedia page - https://en.wikipedia.org/wiki/New_York_City"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Download all the dependencies that is needed.**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import numpy as np # library to handle data in a vectorized manner\n\nimport pandas as pd # library for data analsysis\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport json # library to handle JSON files\n\n!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\n\n# conda install -c anaconda beautiful-soup --yes\nfrom bs4 import BeautifulSoup # package for parsing HTML and XML documents\n\nimport csv # implements classes to read and write tabular data in CSV form\n\nprint('Libraries imported.')",
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Solving environment: - ",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Web scrapping of Population data from wikipedia page using BeautifulSoup.**"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "website_url = requests.get('https://en.wikipedia.org/wiki/Demographics_of_New_York_City').text\nsoup = BeautifulSoup(website_url,'lxml')\ntable = soup.find('table',{'class':'wikitable sortable'})\n#print(soup.prettify())\n\nheaders = [header.text for header in table.find_all('th')]\n\ntable_rows = table.find_all('tr')        \nrows = []\nfor row in table_rows:\n   td = row.find_all('td')\n   row = [row.text for row in td]\n   rows.append(row)\n\nwith open('BON2_POPULATION1.csv', 'w') as f:\n   writer = csv.writer(f)\n   writer.writerow(headers)\n   writer.writerows(row for row in rows if row)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Load data from CSV**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Pop_data=pd.read_csv('BON2_POPULATION1.csv')\nPop_data.drop(Pop_data.columns[[7,8,9,10,11]], axis=1,inplace=True)\nprint('Data downloaded!')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Remove whitespaces and rename columns**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Pop_data.columns = Pop_data.columns.str.replace(' ', '')\nPop_data.columns = Pop_data.columns.str.replace('\\'','')\nPop_data.rename(columns={'Borough':'persons_sq_mi','County':'persons_sq_km'}, inplace=True)\nPop_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Pop_data.rename(columns = {'NewYorkCitysfiveboroughsvte\\n' : 'Borough',\n                   'Jurisdiction\\n':'County',\n                   'Population\\n':'Estimate_2017', \n                   'Landarea\\n':'square_miles',\n                    'Density\\n':'square_km'}, inplace=True)\nPop_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Replace newline('\\n') from each string from left and right sides**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Pop_data['Borough']=Pop_data['Borough'].replace(to_replace='\\n', value='', regex=True)\nPop_data['County']=Pop_data['County'].replace(to_replace='\\n', value='', regex=True)\nPop_data['Estimate_2017']=Pop_data['Estimate_2017'].replace(to_replace='\\n', value='', regex=True)\nPop_data['square_miles']=Pop_data['square_miles'].replace(to_replace='\\n', value='', regex=True)\nPop_data['square_km']=Pop_data['square_km'].replace(to_replace='\\n', value='', regex=True)\nPop_data['persons_sq_mi']=Pop_data['persons_sq_mi'].replace(to_replace='\\n', value='', regex=True)\nPop_data['persons_sq_km']=Pop_data['persons_sq_km'].replace(to_replace='\\n', value='', regex=True)\nPop_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Shift data in the last two rows**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Pop_data.loc[5:,['persons_sq_mi','persons_sq_km']] = Pop_data.loc[2:,['persons_sq_mi','persons_sq_km']].shift(1,axis=1)\nPop_data.loc[5:,['square_km','persons_sq_mi']] = Pop_data.loc[2:,['square_km','persons_sq_mi']].shift(1,axis=1)\nPop_data.loc[5:,['square_miles','square_km']] = Pop_data.loc[2:,['square_miles','square_km']].shift(1,axis=1)\nPop_data.loc[5:,['Estimate_2017','square_miles']] = Pop_data.loc[2:,['Estimate_2017','square_miles']].shift(1,axis=1)\nPop_data.loc[5:,['County','Estimate_2017']] = Pop_data.loc[2:,['County','Estimate_2017']].shift(1,axis=1)\nPop_data.loc[5:,['Borough','County']] = Pop_data.loc[2:,['Borough','County']].shift(1,axis=1)\nPop_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Remove 'NAN'**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Pop_data = Pop_data.fillna('')\nPop_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Drop the last row**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "i = Pop_data[((Pop_data.County == 'Sources: [2] and see individual borough articles'))].index\nPop_data.drop(i)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Save dataframe as csv file**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Pop_data.to_csv('BON2_POPULATION.csv',index=False)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**B : DEMOGRAPHICS DATA**"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "We will web scrap Demographics data from wikipedia page - https://en.wikipedia.org/wiki/New_York_City"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Web scrapping of Demographics data from wikipedia page using BeautifulSoup**"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "website_url = requests.get('https://en.wikipedia.org/wiki/New_York_City').text\nsoup = BeautifulSoup(website_url,'lxml')\ntable = soup.find('table',{'class':'wikitable sortable collapsible'})\n#print(soup.prettify())\n\nheaders = [header.text for header in table.find_all('th')]\n\ntable_rows = table.find_all('tr')        \nrows = []\nfor row in table_rows:\n   td = row.find_all('td')\n   row = [row.text for row in td]\n   rows.append(row)\n\nwith open('NYC_DEMO.csv', 'w') as f:\n   writer = csv.writer(f)\n   writer.writerow(headers)\n   writer.writerows(row for row in rows if row)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Load data from CSV**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data=pd.read_csv('NYC_DEMO.csv')\nprint('Data downloaded!')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Remove whitespaces and rename columns**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data.columns",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data.rename(columns = {'2010[237]' : '2010',\n                   '1990[239]':'1990',\n                   '1970[239]':'1970', \n                   '1940[239]\\n':'1940',\n                    }, inplace=True)\nDemo_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data.columns",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data.columns = Demo_data.columns.str.replace(' ', '')",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Replace newline('\\n') from each string from left and right sides**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data= Demo_data.replace('\\n',' ', regex=True)\nDemo_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Strip '[240]' from third column - 1970**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data['1970'] = Demo_data['1970'].str.rstrip('[240]')\nDemo_data",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "**Save dataframe as csv file**"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "Demo_data.to_csv('BON2_DEMOGRAPHICS.csv',index=False)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}